{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Wrangling-Report\">Wrangling Report<a class=\"anchor-link\" href=\"#Wrangling-Report\">¶</a></h1><ul>\n",
    "<li>First of all, I learned that before making any analyses of the data given, data wrangling is an important aspect to be completed thoroughly, so that none of the analyses go wrong. \n",
    "The Gathering, Assessing and Cleaning were the main three steps done by me in data wrangling.<br/></li>\n",
    "<li>Since these steps are iterative i.e. there is no specific order and I can go back any step whenever I want to, this made my task somewhat easy.<br/></li>\n",
    "<li>While <code>Gathering</code> the data, the first two files, <code>twitter_archive_enhanced.csv</code> was given in the project details as a file on hand but the second <code>image_predictions.tsv</code> file was supposed to be downloaded programmatically which was somewhat an easy task to do. But the third part of querying the twitter API to gather the additional_data from the account @weratedaogs about retweet_counts and favorite_counts was a bit tough for me. It took me time to understand and solve the given problem, although once I got it I was happy that I could do it.<br/></li>\n",
    "<li><p>Now in the <code>Assessing</code> part I had to assess the files gathered by me by using two ways i.e. Visually and Programmatically. For Visual Assessment I used the Numbers application on the Mac\n",
    "to view the sheets. While for the Programmatic Assessment I used the info(), value_counts(), head() etc, such methods as well as I used some conditions to check the dataset. Assessing was done in two steps:-<br/> Step(1) Detect issues.<br/> Step(2) Document the detected issues. The following were the issues assessed and cleaned by me:<br/>\n",
    "<h2>Quality</h2></p>\n",
    "</li>\n",
    "<li><p>(1) In <code>df1</code>, 'timestamp' datatype is object.<br/></p>\n",
    "</li>\n",
    "<li>(2) In <code>df1</code>, 'retweeted_status_timestamp' datatype is object.<br/></li>\n",
    "<li>(3) In <code>df1</code>, 'rating_numerator' has not all ratings correct.<br/></li>\n",
    "<li>(4) In <code>df1</code>, 'rating_denominator', all the denominators must be 10.<br/> </li>\n",
    "<li>(5) 'Emmy','Shadow', 'Sierra', 'Canela' records are duplicated.<br/></li>\n",
    "<li>(6) In <code>df1</code>, the 'name' column has some wrong names like a, an, the, this, quite, such, etc.<br/></li>\n",
    "<li>(7) In <code>df1</code>, 'in_reply_to_user_id' datatype is float.<br/></li>\n",
    "<li>(8) In <code>df1</code>, 'retweeted_status_user_id' datatype is float.<br/></li>\n",
    "<li>(9) In <code>df1</code>, sometimes 'retweeted_status_user_id' is 9 digits other times 10 digits.<br/></li>\n",
    "<li>(10) <code>retweet_count</code> &amp; <code>favorite_count</code> are datatype float, <code>tweet_id</code>, <code>in_reply_to_status_id</code>, <code>retweeted_status_id</code> are datatype int/float.</li>\n",
    "<li>(11) There are some rows representing retweets.</li>\n",
    "<li>(12) One of the ratings are decimal like 9.5/10 (tweet_id = 681340665377193984) but is not printed likewise.</li>\n",
    "<li><p>Other is 13.5/10 (tweet_id = 883482846933004288) but is not printed likewise.<br/>\n",
    "<h2> Tidiness</h2></p>\n",
    "</li>\n",
    "<li><p>(1) <code>df</code> and <code>df2</code> should be part of <code>df1</code>.<br/></p>\n",
    "</li>\n",
    "<li>(2) The 'text' in <code>df1</code> contains the ratings and urls alongwith the text.<br/></li>\n",
    "<li>(3) There are multiple columns for one variable i.e. dog_stages.<br/><br/></li>\n",
    "<li>I looked for the Data Quality Dimensions like Completeness, Validity, Accuracy and Consistency while <code>Assessing</code> and <code>Cleaning</code> the dataset. <br/> Also Quality and tidiness issues were documented separately.</li>\n",
    "<li>After Assessing, my next hurdle was to <code>Clean</code> the detected issues in the dataset. The cleaning of the dataset was done by me according to the rules of tidy data i.e. satisfying those rules required for a dataset to be a tidy one. The cleaning of the dataset was done using various pandas functions and using conditions. The Cleaning was done in three (3) steps viz. —Define—Code—Test— <br/></li>\n",
    "<li>I then stored the cleaned dataset into the <code>twitter_archive_master.csv</code> file thus completing the data wrangling work followed by analysing and visualising the dataset.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
